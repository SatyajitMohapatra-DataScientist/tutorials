{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Serving\n",
    " --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model created in the previous notebook and predict the output of the data coming from `serving-stream`. The model execution and results are tracked using MLRun.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import load_project, new_project, mount_v3io\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/rapid-streaming-ml/conf\n",
      "Project name: model-deployment-with-streaming-michaelk\n"
     ]
    }
   ],
   "source": [
    "project_name = '-'.join(filter(None, [PROJECT_NAME, getenv('V3IO_USERNAME', None)]))\n",
    "project_path = path.abspath('conf')\n",
    "\n",
    "if path.isfile(path.join(project_path, 'project.yaml')):\n",
    "    project = load_project(name=project_name, context=project_path, init_git=False)\n",
    "else:\n",
    "    project = new_project(project_name, project_path, init_git=True)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set model path:  /User/examples/model-deployment-with-streaming/model.pkl\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "MODEL_PATH = path.join('/User/examples', PROJECT_NAME, 'model.pkl')\n",
    "print(f'Set model path:  {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Serving Functions from the MLRun Functions Marketplace <a id=\"gs-ml-pipeline-add-functions\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_function('hub://model_server:development', 'serving')\n",
    "\n",
    "input_stream = f'http://v3io-webapi:8081/users/{getenv(\"V3IO_USERNAME\")}/examples/rapid-prototype/serving-stream@modelserver'\n",
    "partitions = list(range(0,8))\n",
    "\n",
    "serving = project.func('serving').apply(mount_v3io())\n",
    "serving.add_model(MODEL_NAME, MODEL_PATH)\n",
    "serving.set_envs({'INFERENCE_STREAM' : path.join('users', STREAM_CONFIGS['inference-stream']['path']) })\n",
    "\n",
    "serving.add_trigger('serving_stream', nuclio.triggers.V3IOStreamTrigger(url=input_stream, partitions=partitions))\n",
    "serving.spec.config.pop('spec.triggers.http')\n",
    "\n",
    "serving.spec.min_replicas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-08-06 13:09:35,432 [info] deploy started\n",
      "[nuclio] 2020-08-06 13:09:37,561 (info) Build complete\n",
      "[nuclio] 2020-08-06 13:09:49,713 (info) Function deploy complete\n",
      "[nuclio] 2020-08-06 13:09:49,721 done updating model-deployment-with-streaming-michaelk-sklearn-server, function address: 192.168.226.12:31932\n"
     ]
    }
   ],
   "source": [
    "serving_addr = serving.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the seving func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "feat = [7,799,47,17,143,1560,7,164,11,810.0,162.0,5.0,782.0]\n",
    "json = json.dumps({'instances': np.array(feat).reshape(1,-1).tolist()})\n",
    "\n",
    "resp = requests.post(url=f'{serving_addr}/{MODEL_NAME}/predict', \n",
    "                     json = json)\n",
    "print(resp.status_code)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save(path.join(project_path, 'project.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
