{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Serving\n",
    " --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model created in the previous notebook and predict the output of the data coming from `serving-stream`. The model execution and results are tracked using MLRun.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import load_project\n",
    "from os import path\n",
    "\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEB_API_USERS = project.params.get('WEB_API_USERS')\n",
    "serving_stream_path = project.params.get('STREAM_CONFIGS').get('serving-stream').get('path')\n",
    "inference_stream_path = project.params.get('STREAM_CONFIGS').get('inference-stream').get('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set model path:  /User/examples/model-deployment-with-streaming/model.pkl\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "MODEL_PATH = path.join('/User/examples', project.params.get('PROJECT_BASE_NAME'), 'model.pkl')\n",
    "print(f'Set model path:  {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Serving Functions from the MLRun Functions Marketplace <a id=\"gs-ml-pipeline-add-functions\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to your project a real-time model-server function (`serving`) via the [`model_server`](https://github.com/mlrun/functions/tree/master/model_server) MLRun marketplace function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f2496486090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nuclio\n",
    "from mlrun import mount_v3io\n",
    "project.set_function('hub://model_server:development', 'serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the serving function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the trained model, input stream and inference stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_stream = '/'.join(s.strip('/') for s in [WEB_API_USERS, serving_stream_path]) + '@modelserver'\n",
    "partitions = list(range(0,8))\n",
    "\n",
    "serving = project.func('serving').apply(mount_v3io())\n",
    "serving.add_model(MODEL_NAME, MODEL_PATH)\n",
    "serving.set_envs({'INFERENCE_STREAM' : path.join('users', inference_stream_path) })\n",
    "\n",
    "serving.add_trigger('serving_stream', nuclio.triggers.V3IOStreamTrigger(url=input_stream, partitions=partitions))\n",
    "serving.spec.config.pop('spec.triggers.http')\n",
    "\n",
    "serving.spec.min_replicas = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-08-20 11:17:03,730 [info] deploy started\n",
      "[nuclio] 2020-08-20 11:17:05,861 (info) Build complete\n",
      "[nuclio] 2020-08-20 11:17:12,996 done creating model-deployment-with-streaming-iguazio-sklearn-server, function address: 3.131.87.251:30742\n"
     ]
    }
   ],
   "source": [
    "serving_addr = serving.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "feat = [7,799,47,17,143,1560,7,164,11,810.0,162.0,5.0,782.0]\n",
    "json = json.dumps({'instances': np.array(feat).reshape(1,-1).tolist()})\n",
    "\n",
    "resp = requests.post(url=f'{serving_addr}/{MODEL_NAME}/predict', \n",
    "                     json = json)\n",
    "print(resp.status_code)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
