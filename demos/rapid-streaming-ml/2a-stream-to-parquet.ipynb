{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Stream to Parquet\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the input stream to a set of parquet files. The purpose is to store the input stream to a log of raw events.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio env BATCH_SIZE = ${MDWS_S2P_BATCH_SIZE}\n",
    "%nuclio env TARGET_PATH = ${MDWS_S2P_TARGET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function spec\n",
    "%nuclio config kind = \"nuclio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "\n",
    "python -m pip install pandas\n",
    "python -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio config\n",
    "spec.readinessTimeoutSeconds = 200\n",
    "spec.triggers.v3io_stream.kind = \"v3ioStream\"\n",
    "spec.triggers.v3io_stream.disabled = false\n",
    "spec.triggers.v3io_stream.url = \"${MDWS_INPUT_URL}\"\n",
    "spec.triggers.v3io_stream.maxWorkers = 10\n",
    "spec.triggers.v3io_stream.password = \"${V3IO_ACCESS_KEY}\"\n",
    "spec.triggers.v3io_stream.attributes.pollingIntervalMs = 500\n",
    "spec.triggers.v3io_stream.attributes.seekTo = \"earliest\"\n",
    "spec.triggers.v3io_stream.attributes.readBatchSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio mount /User ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    setattr(context, 'batch', [])\n",
    "    setattr(context, 'batch_size', int(os.getenv('BATCH_SIZE', 1024)))\n",
    "    setattr(context, 'batch_count',int(os.getenv('BATCH_COUNT', 0)))\n",
    "    \n",
    "    pq_partitions = os.getenv('PQ_PARTITIONS')\n",
    "    if pq_partitions:\n",
    "        setattr(context, 'pq_partitions', pq_partitions.split(','))\n",
    "    else:\n",
    "        setattr(context, 'pq_partitions', pq_partitions)\n",
    "    \n",
    "    setattr(context, 'target_path', os.getenv('TARGET_PATH'))\n",
    "    os.makedirs(context.target_path, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    if type(event.body) is dict:\n",
    "        event_dict = event.body\n",
    "    else:\n",
    "        event_dict = json.loads(event.body)\n",
    "        \n",
    "    context.logger.info_with('Got invoked',\n",
    "                             trigger_kind=event.trigger.kind,\n",
    "                             event_body=event_dict)\n",
    "    \n",
    "    # add the incoming event to the current batch\n",
    "    context.batch.append(event_dict)\n",
    "    \n",
    "    #check if batch size reached\n",
    "    if context.batch_size == len(context.batch):\n",
    "        context.logger.info_with('Writing batch',\n",
    "                                 batch_count=context.batch_count,\n",
    "                                 batch_size=len(context.batch))\n",
    "        write_batch(context)\n",
    "        context.logger.info_with('Written batch',\n",
    "                                 batch_count=context.batch_count,\n",
    "                                 batch_size=len(context.batch))\n",
    "        \n",
    "        \n",
    "def write_batch(context):\n",
    "    file_name = str(context.worker_id)+'_'+str(context.batch_count)\n",
    "    df = pd.DataFrame.from_records(context.batch)\n",
    "    df.to_parquet(path=os.path.join(context.target_path, file_name), partition_cols=context.pq_partitions)\n",
    "    # post write cleanup and counter update\n",
    "    context.batch = []\n",
    "    context.batch_count += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_context(context)\n",
    "#reduce the batch size to 10\n",
    "context.batch_size = 10\n",
    "\n",
    "# trigger with 9 events:\n",
    "\n",
    "nine_events = [b'{\"user_id\" : 1 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 2 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 3 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 4 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 5 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 6 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 7 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 8 , \"event_type\": \"spin\"}',\n",
    "              b'{\"user_id\" : 9 , \"event_type\": \"spin\"}']\n",
    "\n",
    "for e in nine_events:\n",
    "    event = nuclio.Event(body=e)\n",
    "    handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weather a parquet has been created\n",
    "!ls -l ${TARGET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger the tenth event which should trigger the creation of the parquet file.\n",
    "tenth_event = b'{\"user_id\" : 10 , \"event_type\": \"spin\"}'\n",
    "event = nuclio.Event(body=tenth_event)\n",
    "handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weather a parquet has been created\n",
    "!ls -l ${TARGET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!rm ${TARGET_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio deploy -p ${MDWS_PROJECT_NAME} -n ${V3IO_USERNAME}-stream-to-parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
