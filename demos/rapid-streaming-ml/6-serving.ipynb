{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Serving\n",
    " --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model created in the previous notebook and predict the output of the data coming from `serving-stream`. The model execution and results are tracked using MLRun.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import load_project, mount_v3io\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/rapid-proto/conf\n",
      "Project name: rapid-prototype-michaelk\n"
     ]
    }
   ],
   "source": [
    "project_name = '-'.join(filter(None, [rapid, getenv('V3IO_USERNAME', None)]))\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(name=project_name, context=project_path, init_git=False)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Serving Functions from the MLRun Functions Marketplace <a id=\"gs-ml-pipeline-add-functions\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_function('hub://model_server:development', 'serving')\n",
    "\n",
    "input_stream = f'http://v3io-webapi:8081/users/{getenv(\"V3IO_USERNAME\")}/examples/rapid-prototype/serving-stream@modelserver'\n",
    "partitions = list(range(0,8))\n",
    "\n",
    "serving = project.func('serving').apply(mount_v3io())\n",
    "serving.add_model('rapid_proto', '/User/rapid-proto/artifacts/model/3/model.pkl')\n",
    "serving.set_envs({'INFERENCE_STREAM' : path.join('users', getenv('V3IO_USERNAME'), 'examples/rapid-prototype/inference-stream')})\n",
    "\n",
    "serving.add_trigger('serving_stream', nuclio.triggers.V3IOStreamTrigger(url=input_stream, partitions=partitions))\n",
    "serving.spec.config.pop('spec.triggers.http')\n",
    "\n",
    "serving.spec.min_replicas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-08-03 16:33:54,338 [info] deploy started\n",
      "[nuclio] 2020-08-03 16:35:09,589 (info) Build complete\n",
      "[nuclio] 2020-08-03 16:35:28,852 done creating rapid-prototype-mk-sklearn-server, function address: 192.168.226.12:32259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://192.168.226.12:32259'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_addr = serving.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the seving func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "feat = [7,799,47,17,143,1560,7,164,11,810.0,162.0,5.0,782.0]\n",
    "json = json.dumps({'instances': np.array(feat).reshape(1,-1).tolist()})\n",
    "\n",
    "resp = requests.post(url=f'{serving_addr}/rapid_proto/predict', \n",
    "                     json = json)\n",
    "print(resp.status_code)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
